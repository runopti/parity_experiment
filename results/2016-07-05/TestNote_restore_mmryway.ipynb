{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "args = edict({'hidden_size':1, 'batch_size':1, 'output_size': 2, 'seq_len': 3, 'train_method': \"single\", 'max_grad_norm':5.0, 'grad_clip': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = args.hidden_size # 2 # 4\n",
    "batch_size = args.batch_size #1\n",
    "output_size = args.output_size  #2\n",
    "seq_len = args.seq_len  #12 #3  #6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    ############## Graph construction ################\n",
    "    data = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "    if args.train_method == \"single\":\n",
    "        target = tf.placeholder(tf.float32, shape=[batch_size, output_size])\n",
    "    else:\n",
    "        target = tf.placeholder(tf.float32, shape=[seq_len, batch_size, output_size])\n",
    "        target_list = [tf.squeeze(tf.slice(target, [i,0,0], [1, batch_size, output_size])) for i in range(seq_len)]\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    lstm = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    # initialize the state\n",
    "    initial_state = state = tf.zeros([batch_size, hidden_size])\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if i > 0: tf.get_variable_scope().reuse_variables()\n",
    "        with tf.name_scope(\"in\") as scope:\n",
    "            weights = tf.Variable(tf.truncated_normal([hidden_size, batch_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"weights\"))\n",
    "            biases = tf.Variable(tf.truncated_normal([hidden_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"biases\"))\n",
    "            mapped = weights *  data[:,i]\n",
    "            mapped = tf.reshape(mapped, [1, hidden_size]) # 1 x hidden_size\n",
    "            mapped = mapped + biases\n",
    "            #mapped = tf.Print(mapped, [mapped], message=\"this is mapped: \")\n",
    "        cell_output, state = lstm(mapped, state)\n",
    "        with tf.name_scope(\"out\") as scope:\n",
    "            weights = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"weights\"))\n",
    "            biases = tf.Variable(tf.truncated_normal([output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"biases\"))\n",
    "            output = tf.nn.softmax(tf.matmul(cell_output, weights) + biases)  # the size of output should be just [batch_size, 1] right?\n",
    "            #output = tf.reshape(output, [1,2])\n",
    "        if args.train_method == \"single\":                                                    \n",
    "            pass\n",
    "        else:\n",
    "            # target has to be shape=[seq_len * [1,2]]\n",
    "            loss_per_digit = -tf.reduce_sum(target_list[i]*tf.log(output)) # this should be just 1 by 1 - 1 by 1            \n",
    "            if args.grad_clip:\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(loss_per_digit, tvars), args.max_grad_norm)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            else:\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    if args.train_method == \"single\":\n",
    "        loss = -tf.reduce_sum(target*tf.log(output)) # this should be just 1 by 1 - 1 by 1\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "        #train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "        if args.grad_clip:\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), args.max_grad_norm)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        else:\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # 0.001\n",
    "    else:\n",
    "        loss = tf.identity(loss_per_digit)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ############### Graph construction end ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        #new_saver = tf.train.import_meta_graph('./hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0.meta')\n",
    "        #print(new_saver.GraphDef())\n",
    "        saver.restore(sess, './hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0')\n",
    "        #print(ops.get_default_graph().as_graph_def())\n",
    "        #graph_def = ops.get_default_graph().as_graph_def()\n",
    "        \n",
    "        #var_dict = {}\n",
    "        #var_name_list = []\n",
    "        #for var in tf.trainable_variables():\n",
    "         #   print(var.value().name)\n",
    "              #print(var.eval())\n",
    "            # var_dict[var.value().name] = sess.run(var) #tf.constant(var.eval())  # was just var.name before\n",
    "            #var_name_list.append(var.value().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81977516]]\n",
      "[[-1.45694232]]\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    #graph_def = g1.as_graph_def()\n",
    "    #for node in graph_def.node:\n",
    "        #print(node.name)\n",
    "        #print(node.attr['value'].tensor)\n",
    "        #if node.name == \"in_1/weights\":\n",
    "          #  print(\"in_1/weights\")\n",
    "            #print(node.attr['value'].tensor)\n",
    "        #if node.name == \"in_2/weights\":\n",
    "          #  print(\"in_2/weights\")\n",
    "            #print(node.attr['value'].tensor)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for var in tf.trainable_variables():\n",
    "                #print(var.name)\n",
    "                if var.name == \"in_1/Variable:0\":\n",
    "                    print(var.eval())\n",
    "                if var.name == \"in_2/Variable:0\":\n",
    "                    print(var.eval())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97308564 -0.54189807]\n",
      "[-1.36001205  1.28313386]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph('./hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0.meta')\n",
    "        #print(new_saver.GraphDef())\n",
    "        new_saver.restore(sess, './hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0')\n",
    "        #print(ops.get_default_graph().as_graph_def())\n",
    "        #graph_def = ops.get_default_graph().as_graph_def()\n",
    "        \n",
    "        for var in tf.trainable_variables():\n",
    "            #print(var.name)\n",
    "            if var.name == \"out_1/Variable_1:0\":\n",
    "                print(var.eval())\n",
    "            if var.name == \"out_2/Variable_1:0\":\n",
    "                print(var.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in/Variable:0\n",
      "in/Variable_1:0\n",
      "BasicRNNCell/Linear/Matrix:0\n",
      "BasicRNNCell/Linear/Bias:0\n",
      "out/Variable:0\n",
      "out/Variable_1:0\n",
      "in_1/Variable:0\n",
      "in_1/Variable_1:0\n",
      "out_1/Variable:0\n",
      "out_1/Variable_1:0\n",
      "in_2/Variable:0\n",
      "in_2/Variable_1:0\n",
      "out_2/Variable:0\n",
      "out_2/Variable_1:0\n"
     ]
    }
   ],
   "source": [
    "with g2.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        for var in tf.trainable_variables():\n",
    "            print(var.name)\n",
    "            #if var.name == \"in_1/Variable:0\":\n",
    "             #   print(var.eval())\n",
    "            #if var.name == \"in_2/Variable:0\":\n",
    "              #  print(var.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g3 = tf.Graph()\n",
    "with g3.as_default():\n",
    "    ############## Graph construction ################\n",
    "    data = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "    if args.train_method == \"single\":\n",
    "        target = tf.placeholder(tf.float32, shape=[batch_size, output_size])\n",
    "    else:\n",
    "        target = tf.placeholder(tf.float32, shape=[seq_len, batch_size, output_size])\n",
    "        target_list = [tf.squeeze(tf.slice(target, [i,0,0], [1, batch_size, output_size])) for i in range(seq_len)]\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    # initialize the state\n",
    "    initial_state = state = tf.zeros([batch_size, hidden_size])\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if i > 0: tf.get_variable_scope().reuse_variables()      \n",
    "        cell_output, state = rnn_cell(tf.reshape(data[:,i], [1,1]), state)\n",
    "        output = tf.nn.softmax(cell_output)  # the size of output should be just [batch_size, 1] right?\n",
    "        \n",
    "        if args.train_method == \"single\":                                                    \n",
    "            pass\n",
    "        else:\n",
    "            # target has to be shape=[seq_len * [1,2]]\n",
    "            loss_per_digit = -tf.reduce_sum(target_list[i]*tf.log(output)) # this should be just 1 by 1 - 1 by 1            \n",
    "            if args.grad_clip:\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(loss_per_digit, tvars), args.max_grad_norm)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            else:\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    if args.train_method == \"single\":\n",
    "        loss = -tf.reduce_sum(target*tf.log(output)) # this should be just 1 by 1 - 1 by 1\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "        #train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "        if args.grad_clip:\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), args.max_grad_norm)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        else:\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # 0.001\n",
    "    else:\n",
    "        loss = tf.identity(loss_per_digit)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ############### Graph construction end ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicRNNCell/Linear/Matrix:0\n",
      "(2, 1)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10d7f97b8>\n",
      "BasicRNNCell/Linear/Bias:0\n",
      "(1,)\n",
      "<tensorflow.python.framework.ops.Graph object at 0x10d7f97b8>\n"
     ]
    }
   ],
   "source": [
    "with g3.as_default():\n",
    "    for var in tf.trainable_variables():\n",
    "        #print(var.name)\n",
    "        print(var.name)\n",
    "        print(var.get_shape())\n",
    "        print(var.graph)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
