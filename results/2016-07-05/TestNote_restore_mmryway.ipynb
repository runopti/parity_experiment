{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict\n",
    "args = edict({'hidden_size':1, 'batch_size':1, 'output_size': 2, 'seq_len': 3, 'train_method': \"single\", 'max_grad_norm':5.0, 'grad_clip': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size = args.hidden_size # 2 # 4\n",
    "batch_size = args.batch_size #1\n",
    "output_size = args.output_size  #2\n",
    "seq_len = args.seq_len  #12 #3  #6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    ############## Graph construction ################\n",
    "    data = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "    if args.train_method == \"single\":\n",
    "        target = tf.placeholder(tf.float32, shape=[batch_size, output_size])\n",
    "    else:\n",
    "        target = tf.placeholder(tf.float32, shape=[seq_len, batch_size, output_size])\n",
    "        target_list = [tf.squeeze(tf.slice(target, [i,0,0], [1, batch_size, output_size])) for i in range(seq_len)]\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    lstm = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    # initialize the state\n",
    "    initial_state = state = tf.zeros([batch_size, hidden_size])\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if i > 0: tf.get_variable_scope().reuse_variables()\n",
    "        with tf.name_scope(\"in\") as scope:\n",
    "            weights = tf.Variable(tf.truncated_normal([hidden_size, batch_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"weights\"))\n",
    "            biases = tf.Variable(tf.truncated_normal([hidden_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"biases\"))\n",
    "            mapped = weights *  data[:,i]\n",
    "            mapped = tf.reshape(mapped, [1, hidden_size]) # 1 x hidden_size\n",
    "            mapped = mapped + biases\n",
    "            #mapped = tf.Print(mapped, [mapped], message=\"this is mapped: \")\n",
    "        cell_output, state = lstm(mapped, state)\n",
    "        with tf.name_scope(\"out\") as scope:\n",
    "            weights = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"weights\"))\n",
    "            biases = tf.Variable(tf.truncated_normal([output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"biases\"))\n",
    "            output = tf.nn.softmax(tf.matmul(cell_output, weights) + biases)  # the size of output should be just [batch_size, 1] right?\n",
    "            #output = tf.reshape(output, [1,2])\n",
    "        if args.train_method == \"single\":                                                    \n",
    "            pass\n",
    "        else:\n",
    "            # target has to be shape=[seq_len * [1,2]]\n",
    "            loss_per_digit = -tf.reduce_sum(target_list[i]*tf.log(output)) # this should be just 1 by 1 - 1 by 1            \n",
    "            if args.grad_clip:\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(loss_per_digit, tvars), args.max_grad_norm)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            else:\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    if args.train_method == \"single\":\n",
    "        loss = -tf.reduce_sum(target*tf.log(output)) # this should be just 1 by 1 - 1 by 1\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "        #train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "        if args.grad_clip:\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), args.max_grad_norm)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        else:\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # 0.001\n",
    "    else:\n",
    "        loss = tf.identity(loss_per_digit)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ############### Graph construction end ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    saver = tf.train.Saver(tf.all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with g1.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        #new_saver = tf.train.import_meta_graph('./hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0.meta')\n",
    "        #print(new_saver.GraphDef())\n",
    "        saver.restore(sess, './hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0')\n",
    "        #print(ops.get_default_graph().as_graph_def())\n",
    "        #graph_def = ops.get_default_graph().as_graph_def()\n",
    "        \n",
    "        #var_dict = {}\n",
    "        #var_name_list = []\n",
    "        #for var in tf.trainable_variables():\n",
    "         #   print(var.value().name)\n",
    "              #print(var.eval())\n",
    "            # var_dict[var.value().name] = sess.run(var) #tf.constant(var.eval())  # was just var.name before\n",
    "            #var_name_list.append(var.value().name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.81977516]]\n",
      "[[-1.45694232]]\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    #graph_def = g1.as_graph_def()\n",
    "    #for node in graph_def.node:\n",
    "        #print(node.name)\n",
    "        #print(node.attr['value'].tensor)\n",
    "        #if node.name == \"in_1/weights\":\n",
    "          #  print(\"in_1/weights\")\n",
    "            #print(node.attr['value'].tensor)\n",
    "        #if node.name == \"in_2/weights\":\n",
    "          #  print(\"in_2/weights\")\n",
    "            #print(node.attr['value'].tensor)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            for var in tf.trainable_variables():\n",
    "                #print(var.name)\n",
    "                if var.name == \"in_1/Variable:0\":\n",
    "                    print(var.eval())\n",
    "                if var.name == \"in_2/Variable:0\":\n",
    "                    print(var.eval())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.97308564 -0.54189807]\n",
      "[-1.36001205  1.28313386]\n"
     ]
    }
   ],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        new_saver = tf.train.import_meta_graph('./hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0.meta')\n",
    "        #print(new_saver.GraphDef())\n",
    "        new_saver.restore(sess, './hidden-uni-vis-test-2016-07-07-09:10:05/my_model-0')\n",
    "        #print(ops.get_default_graph().as_graph_def())\n",
    "        #graph_def = ops.get_default_graph().as_graph_def()\n",
    "        \n",
    "        for var in tf.trainable_variables():\n",
    "            #print(var.name)\n",
    "            if var.name == \"out_1/Variable_1:0\":\n",
    "                print(var.eval())\n",
    "            if var.name == \"out_2/Variable_1:0\":\n",
    "                print(var.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in/Variable:0\n",
      "in/Variable_1:0\n",
      "BasicRNNCell/Linear/Matrix:0\n",
      "BasicRNNCell/Linear/Bias:0\n",
      "out/Variable:0\n",
      "out/Variable_1:0\n",
      "in_1/Variable:0\n",
      "in_1/Variable_1:0\n",
      "out_1/Variable:0\n",
      "out_1/Variable_1:0\n",
      "in_2/Variable:0\n",
      "in_2/Variable_1:0\n",
      "out_2/Variable:0\n",
      "out_2/Variable_1:0\n"
     ]
    }
   ],
   "source": [
    "with g2.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        for var in tf.trainable_variables():\n",
    "            print(var.name)\n",
    "            #if var.name == \"in_1/Variable:0\":\n",
    "             #   print(var.eval())\n",
    "            #if var.name == \"in_2/Variable:0\":\n",
    "              #  print(var.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g3 = tf.Graph()\n",
    "with g3.as_default():\n",
    "    ############## Graph construction ################\n",
    "    data = tf.placeholder(tf.float32, shape=[batch_size, seq_len])\n",
    "    if args.train_method == \"single\":\n",
    "        target = tf.placeholder(tf.float32, shape=[batch_size, output_size])\n",
    "    else:\n",
    "        target = tf.placeholder(tf.float32, shape=[seq_len, batch_size, output_size])\n",
    "        target_list = [tf.squeeze(tf.slice(target, [i,0,0], [1, batch_size, output_size])) for i in range(seq_len)]\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    # initialize the state\n",
    "    initial_state = state = tf.zeros([batch_size, hidden_size])\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if i > 0: tf.get_variable_scope().reuse_variables()      \n",
    "        cell_output, state = rnn_cell(tf.reshape(data[:,i], [1,1]), state)\n",
    "        weights = tf.get_variable(\"weights\", dtype=tf.float32, initializer=tf.truncated_normal([hidden_size, output_size], stddev=1.0/math.sqrt(float(hidden_size))))\n",
    "        biases = tf.get_variable(\"biases\", dtype=tf.float32,initializer=tf.truncated_normal([output_size], stddev=1.0/math.sqrt(float(hidden_size))))\n",
    "        #weights = tf.Variable(tf.truncated_normal([hidden_size, output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"weights\"))\n",
    "        #biases = tf.Variable(tf.truncated_normal([output_size], stddev=1.0/math.sqrt(float(hidden_size)), name=\"biases\"))\n",
    "        output = tf.nn.softmax(tf.matmul(cell_output, weights) + biases) # the size of output should be just [batch_size, 1] right?\n",
    "        \n",
    "        \n",
    "        if args.train_method == \"single\":                                                    \n",
    "            pass\n",
    "        else:\n",
    "            # target has to be shape=[seq_len * [1,2]]\n",
    "            loss_per_digit = -tf.reduce_sum(target_list[i]*tf.log(output)) # this should be just 1 by 1 - 1 by 1            \n",
    "            if args.grad_clip:\n",
    "                tvars = tf.trainable_variables()\n",
    "                grads, _ = tf.clip_by_global_norm(tf.gradients(loss_per_digit, tvars), args.max_grad_norm)\n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            else:\n",
    "                train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    if args.train_method == \"single\":\n",
    "        loss = -tf.reduce_sum(target*tf.log(output)) # this should be just 1 by 1 - 1 by 1\n",
    "        tf.scalar_summary(\"loss\", loss)\n",
    "        #train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "        if args.grad_clip:\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), args.max_grad_norm)\n",
    "            #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        else:\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # 0.001\n",
    "    else:\n",
    "        loss = tf.identity(loss_per_digit)\n",
    "    \n",
    "    init_op = tf.initialize_all_variables()\n",
    "    ############### Graph construction end ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicRNNCell/Linear/Matrix:0\n",
      "BasicRNNCell/Linear/Bias:0\n",
      "weights:0\n",
      "biases:0\n"
     ]
    }
   ],
   "source": [
    "with g3.as_default():\n",
    "    for var in tf.trainable_variables():\n",
    "        print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder\n",
      "Placeholder_1\n",
      "Placeholder_2\n",
      "zeros\n",
      "Slice/begin\n",
      "Slice/size\n",
      "Slice\n",
      "Squeeze\n",
      "Reshape/shape\n",
      "Reshape\n",
      "BasicRNNCell/Linear/Matrix\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/shape\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/min\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/max\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/RandomUniform\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/sub\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform/mul\n",
      "BasicRNNCell/Linear/Matrix/Initializer/random_uniform\n",
      "BasicRNNCell/Linear/Matrix/Assign\n",
      "BasicRNNCell/Linear/Matrix/read\n",
      "BasicRNNCell/Linear/concat/concat_dim\n",
      "BasicRNNCell/Linear/concat\n",
      "BasicRNNCell/Linear/MatMul\n",
      "BasicRNNCell/Linear/Bias\n",
      "BasicRNNCell/Linear/Bias/Initializer/Const\n",
      "BasicRNNCell/Linear/Bias/Assign\n",
      "BasicRNNCell/Linear/Bias/read\n",
      "BasicRNNCell/add\n",
      "BasicRNNCell/Tanh\n",
      "Softmax\n",
      "Slice_1/begin\n",
      "Slice_1/size\n",
      "Slice_1\n",
      "Squeeze_1\n",
      "Reshape_1/shape\n",
      "Reshape_1\n",
      "BasicRNNCell_1/Linear/concat/concat_dim\n",
      "BasicRNNCell_1/Linear/concat\n",
      "BasicRNNCell_1/Linear/MatMul\n",
      "BasicRNNCell_1/add\n",
      "BasicRNNCell_1/Tanh\n",
      "Softmax_1\n",
      "Slice_2/begin\n",
      "Slice_2/size\n",
      "Slice_2\n",
      "Squeeze_2\n",
      "Reshape_2/shape\n",
      "Reshape_2\n",
      "BasicRNNCell_2/Linear/concat/concat_dim\n",
      "BasicRNNCell_2/Linear/concat\n",
      "BasicRNNCell_2/Linear/MatMul\n",
      "BasicRNNCell_2/add\n",
      "BasicRNNCell_2/Tanh\n",
      "Softmax_2\n",
      "Log\n",
      "mul\n",
      "Rank\n",
      "range/start\n",
      "range/delta\n",
      "range\n",
      "Sum\n",
      "Neg\n",
      "ScalarSummary/tags\n",
      "ScalarSummary\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/Neg_grad/Neg\n",
      "gradients/Sum_grad/Shape\n",
      "gradients/Sum_grad/Size\n",
      "gradients/Sum_grad/add\n",
      "gradients/Sum_grad/mod\n",
      "gradients/Sum_grad/Shape_1\n",
      "gradients/Sum_grad/range/start\n",
      "gradients/Sum_grad/range/delta\n",
      "gradients/Sum_grad/range\n",
      "gradients/Sum_grad/Fill/value\n",
      "gradients/Sum_grad/Fill\n",
      "gradients/Sum_grad/DynamicStitch\n",
      "gradients/Sum_grad/Maximum/y\n",
      "gradients/Sum_grad/Maximum\n",
      "gradients/Sum_grad/floordiv\n",
      "gradients/Sum_grad/Reshape\n",
      "gradients/Sum_grad/Tile\n",
      "gradients/mul_grad/Shape\n",
      "gradients/mul_grad/Shape_1\n",
      "gradients/mul_grad/BroadcastGradientArgs\n",
      "gradients/mul_grad/mul\n",
      "gradients/mul_grad/Sum\n",
      "gradients/mul_grad/Reshape\n",
      "gradients/mul_grad/mul_1\n",
      "gradients/mul_grad/Sum_1\n",
      "gradients/mul_grad/Reshape_1\n",
      "gradients/Log_grad/Inv\n",
      "gradients/Log_grad/mul\n",
      "gradients/Softmax_2_grad/mul\n",
      "gradients/Softmax_2_grad/Sum/reduction_indices\n",
      "gradients/Softmax_2_grad/Sum\n",
      "gradients/Softmax_2_grad/Reshape/shape\n",
      "gradients/Softmax_2_grad/Reshape\n",
      "gradients/Softmax_2_grad/sub\n",
      "gradients/Softmax_2_grad/mul_1\n",
      "gradients/BasicRNNCell_2/Tanh_grad/Square\n",
      "gradients/BasicRNNCell_2/Tanh_grad/sub/x\n",
      "gradients/BasicRNNCell_2/Tanh_grad/sub\n",
      "gradients/BasicRNNCell_2/Tanh_grad/mul\n",
      "gradients/BasicRNNCell_2/add_grad/Shape\n",
      "gradients/BasicRNNCell_2/add_grad/Shape_1\n",
      "gradients/BasicRNNCell_2/add_grad/BroadcastGradientArgs\n",
      "gradients/BasicRNNCell_2/add_grad/Sum\n",
      "gradients/BasicRNNCell_2/add_grad/Reshape\n",
      "gradients/BasicRNNCell_2/add_grad/Sum_1\n",
      "gradients/BasicRNNCell_2/add_grad/Reshape_1\n",
      "gradients/BasicRNNCell_2/Linear/MatMul_grad/MatMul\n",
      "gradients/BasicRNNCell_2/Linear/MatMul_grad/MatMul_1\n",
      "gradients/BasicRNNCell_2/Linear/concat_grad/ShapeN\n",
      "gradients/BasicRNNCell_2/Linear/concat_grad/ConcatOffset\n",
      "gradients/BasicRNNCell_2/Linear/concat_grad/Slice\n",
      "gradients/BasicRNNCell_2/Linear/concat_grad/Slice_1\n",
      "gradients/BasicRNNCell_1/Tanh_grad/Square\n",
      "gradients/BasicRNNCell_1/Tanh_grad/sub/x\n",
      "gradients/BasicRNNCell_1/Tanh_grad/sub\n",
      "gradients/BasicRNNCell_1/Tanh_grad/mul\n",
      "gradients/BasicRNNCell_1/add_grad/Shape\n",
      "gradients/BasicRNNCell_1/add_grad/Shape_1\n",
      "gradients/BasicRNNCell_1/add_grad/BroadcastGradientArgs\n",
      "gradients/BasicRNNCell_1/add_grad/Sum\n",
      "gradients/BasicRNNCell_1/add_grad/Reshape\n",
      "gradients/BasicRNNCell_1/add_grad/Sum_1\n",
      "gradients/BasicRNNCell_1/add_grad/Reshape_1\n",
      "gradients/BasicRNNCell_1/Linear/MatMul_grad/MatMul\n",
      "gradients/BasicRNNCell_1/Linear/MatMul_grad/MatMul_1\n",
      "gradients/BasicRNNCell_1/Linear/concat_grad/ShapeN\n",
      "gradients/BasicRNNCell_1/Linear/concat_grad/ConcatOffset\n",
      "gradients/BasicRNNCell_1/Linear/concat_grad/Slice\n",
      "gradients/BasicRNNCell_1/Linear/concat_grad/Slice_1\n",
      "gradients/BasicRNNCell/Tanh_grad/Square\n",
      "gradients/BasicRNNCell/Tanh_grad/sub/x\n",
      "gradients/BasicRNNCell/Tanh_grad/sub\n",
      "gradients/BasicRNNCell/Tanh_grad/mul\n",
      "gradients/BasicRNNCell/add_grad/Shape\n",
      "gradients/BasicRNNCell/add_grad/Shape_1\n",
      "gradients/BasicRNNCell/add_grad/BroadcastGradientArgs\n",
      "gradients/BasicRNNCell/add_grad/Sum\n",
      "gradients/BasicRNNCell/add_grad/Reshape\n",
      "gradients/BasicRNNCell/add_grad/Sum_1\n",
      "gradients/BasicRNNCell/add_grad/Reshape_1\n",
      "gradients/BasicRNNCell/Linear/MatMul_grad/MatMul\n",
      "gradients/BasicRNNCell/Linear/MatMul_grad/MatMul_1\n",
      "gradients/AddN\n",
      "gradients/AddN_1\n",
      "global_norm/L2Loss\n",
      "global_norm/L2Loss_1\n",
      "global_norm/pack\n",
      "global_norm/Rank\n",
      "global_norm/range/start\n",
      "global_norm/range/delta\n",
      "global_norm/range\n",
      "global_norm/Sum\n",
      "global_norm/Const\n",
      "global_norm/mul\n",
      "global_norm/global_norm\n",
      "clip_by_global_norm/truediv/x\n",
      "clip_by_global_norm/truediv\n",
      "clip_by_global_norm/Const\n",
      "clip_by_global_norm/Minimum\n",
      "clip_by_global_norm/mul/x\n",
      "clip_by_global_norm/mul\n",
      "clip_by_global_norm/mul_1\n",
      "clip_by_global_norm/clip_by_global_norm/_0\n",
      "clip_by_global_norm/mul_2\n",
      "clip_by_global_norm/clip_by_global_norm/_1\n",
      "beta1_power/initial_value\n",
      "beta1_power\n",
      "beta1_power/Assign\n",
      "beta1_power/read\n",
      "beta2_power/initial_value\n",
      "beta2_power\n",
      "beta2_power/Assign\n",
      "beta2_power/read\n",
      "zeros_1\n",
      "BasicRNNCell/Linear/Matrix/Adam\n",
      "BasicRNNCell/Linear/Matrix/Adam/Assign\n",
      "BasicRNNCell/Linear/Matrix/Adam/read\n",
      "zeros_2\n",
      "BasicRNNCell/Linear/Matrix/Adam_1\n",
      "BasicRNNCell/Linear/Matrix/Adam_1/Assign\n",
      "BasicRNNCell/Linear/Matrix/Adam_1/read\n",
      "zeros_3\n",
      "BasicRNNCell/Linear/Bias/Adam\n",
      "BasicRNNCell/Linear/Bias/Adam/Assign\n",
      "BasicRNNCell/Linear/Bias/Adam/read\n",
      "zeros_4\n",
      "BasicRNNCell/Linear/Bias/Adam_1\n",
      "BasicRNNCell/Linear/Bias/Adam_1/Assign\n",
      "BasicRNNCell/Linear/Bias/Adam_1/read\n",
      "Adam/beta1\n",
      "Adam/beta2\n",
      "Adam/epsilon\n",
      "Adam/update_BasicRNNCell/Linear/Matrix/ApplyAdam\n",
      "Adam/update_BasicRNNCell/Linear/Bias/ApplyAdam\n",
      "Adam/mul\n",
      "Adam/Assign\n",
      "Adam/mul_1\n",
      "Adam/Assign_1\n",
      "Adam\n",
      "init\n",
      "Identity\n",
      "Identity_1\n",
      "Identity_2\n",
      "Identity_3\n",
      "init_1\n"
     ]
    }
   ],
   "source": [
    "with g3.as_default():\n",
    "    #with tf.Session() as sess:\n",
    "     #   sess.run(tf.initialize_all_variables())\n",
    "    #output.eval()\n",
    "    graph_def = g3.as_graph_def()\n",
    "    for node in graph_def.node:\n",
    "        print(node.name)\n",
    "        #if node.name == \"Softmax\":\n",
    "          #  print(node)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
