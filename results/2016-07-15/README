[1] 5337
LSTM with different seeds.
seq_len15_lstm_seed_diff    -> shitty results.

[1] 6330
run_test.sh

[1] 6623
run_test.sh with hiddenunit 10

[1] 7169
learninf rate test: 0.01

[1][1] 7516 
got stuck with gradient clip 


[1] 7986
hiddenuni

[1] 8316
hiddenuni No.2. Grad clip is now 10.
The loss graph of Hidden_test-2016-07-14-11:21:03 -> zigzag. The model periodically "forgets" what it has learned.

[1] 8663 Hidden_test-2016-07-14-12:21:33
Increased lr. The same setting as above otherwise.

[1] 8972 Hidden_test-2016-07-14-12:49:36
Increased hid_unit from 10 to 20. Lr is back to 0.01 b/c 0.1 didn't decrease the loss at all. 
- the loss still exploded after 400epochs but decreased most among all the other experiments.
- Let's just do another experiment with hidden 30 and then
- investigate the cause of this explosion. (probably learning rate)

[1] 9235  Hidden_test-2016-07-14-13:09:55
Increased hid_unit from 20 to 30. Just set maxepoch 400 to make it shorter.
- didn't really change.

NOTES: Can't we train our model on short sequences (just to learn the switch thing)
and see if it is generalizable to a longer sequence on test phases?

[1] 9529 Short_seq-2016-07-14-13:25:18
seqlen = 4, hidden = 5. Just want to get a model that is trained on short sequences.
Hope this model is generaizable to longer sequences.
- From the look of acc_list.png, it seems the model overfits, so I want to do an early-stopping in the next try.

[1] 9788
maxepoch = 250. The same setting otherwise as above.
- Looks good. Now let's test this on long sequences.
-- The model didn't generalize. I need to train the model that generalizes well.
-- which means I need to investigate the cause of the loss explosion.

[1] 10542 Sigmoid-2016-07-14-16:26:04
sigmoid test. maxepoch=250. This might resolve the problem. 

[1] 10753
Increased maxepoch=800. Increased hiddnunits from 5 to 10.
- The loss sort of scattered towards the end. Why....

[1] 11114 Sigmoid-2016-07-14-17:20:03
Decreased maxepoch=500. Decreased hiddenunits from 10 to 1. 
My hypothesis is that the scattering loss occurs due to the large hidden states.
- The loss didn't change at all.

[1] 11659 Sigmoid-2016-07-14-17:45:21
IMPORTANT: the last three Sigmoid-** experiments are wrong because I was using the old script (rnn_simple.py)
instead of rnn_simple_sigmoid.py
- The loss didn't change at all. I should increase the hidunits from 1 to 5?

[1] 12322 Sigmoid-2016-07-14-18:02:17
Increased hidunits from 1 to 5. 
- The loss is still around 0.63. 

[1] 13146 Sigmoid-2016-07-14-18:24:28
Increased learning_rate from 0.001 to 0.1. 
- The loss didn't change at all. 

[1] 13555 Sigmoid-2016-07-14-18:39:10
Learning rate schedule (divide 10 at epoch 350 based on the observation from 
Sigmoid-2016-07-14-18:02:17 's total_loss plot.
- The loss plot looks worse than Sigmoid-2016-07-14-18:02:17....

[1] 13887
LearningRateSchedule same as above. Increased hiduni from 5 to 30.
- The loss got down to 0.2-0.3, which is surprising, but stopped because of inf.

[1] 14191
Now hiduni 50.
-even better / faster but again the loss went to inf. Learning rate might be too high.

[1] 14469
Learning rate reduction by 10 after 150epoch.
