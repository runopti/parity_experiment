{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(n, output):\n",
    "    seq_len = 4\n",
    "    total_sum = 0 \n",
    "    batch_size = 1\n",
    "    hidden_size = 1\n",
    "    for i in range(n):\n",
    "        state = tf.zeros([batch_size, hidden_size])\n",
    "        x = getData.createInputData(seq_len)\n",
    "        x = np.asarray(x).reshape(1, seq_len)\n",
    "        y = getData.createTargetData(x[0])[-1]\n",
    "        y_target = np.zeros((1,2))\n",
    "        if y == 0: y_target[0][0] = 1 \n",
    "        else: y_target[0][1] = 1 \n",
    "        feed_dict={data:x, target: y_target}\n",
    "        output_ = sess.run(output, feed_dict=feed_dict)\n",
    "        print(output_)\n",
    "        #print(np.argmax(y_target))\n",
    "        #print(np.argmax(output_))\n",
    "        if np.argmax(y_target) == np.argmax(output_):\n",
    "            total_sum += 1\n",
    "    return (1.0 * total_sum / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.31327444  0.68672556]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.36708671  0.63291329]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.25638083  0.74361914]]\n",
      "[[ 0.31327444  0.68672556]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.32099003  0.67900997]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.32099003  0.67900997]]\n",
      "[[ 0.36708671  0.63291329]]\n",
      "[[ 0.26103276  0.7389673 ]]\n",
      "[[ 0.36224982  0.63775021]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.37056398  0.62943602]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.31327444  0.68672556]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.25638083  0.74361914]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.36224982  0.63775021]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.36708671  0.63291329]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.36224982  0.63775021]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.26533335  0.73466671]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.25638083  0.74361914]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.32099003  0.67900997]]\n",
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.36224982  0.63775021]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.25214255  0.74785745]]\n",
      "[[ 0.32099003  0.67900997]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.1763483   0.82365173]]\n",
      "[[ 0.25638083  0.74361914]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.26103276  0.7389673 ]]\n",
      "[[ 0.32516855  0.67483145]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.17121768  0.82878232]]\n",
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.36708671  0.63291329]]\n",
      "[[ 0.32099003  0.67900997]]\n",
      "[[ 0.26103276  0.7389673 ]]\n",
      "[[ 0.31752026  0.6824798 ]]\n",
      "[[ 0.17390585  0.82609415]]\n",
      "[[ 0.36582398  0.63417602]]\n",
      "[[ 0.17918666  0.82081336]]\n",
      "[[ 0.31327444  0.68672556]]\n",
      "0.45\n"
     ]
    }
   ],
   "source": [
    "# # Create a saver.\n",
    "# saver = tf.train.Saver(...variables...)\n",
    "# # Remember the training_op we want to run by adding it to a collection.\n",
    "# tf.add_to_collection('train_op', train_op)\n",
    "# sess = tf.Session()\n",
    "# for step in xrange(1000000):\n",
    "#     sess.run(train_op)\n",
    "#     if step % 1000 == 0:\n",
    "#         # Saves checkpoint, which by default also exports a meta_graph\n",
    "#         # named 'my-model-global_step.meta'.\n",
    "#         saver.save(sess, 'my-model', global_step=step)\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my_model-0.meta')\n",
    "    new_saver.restore(sess, 'my_model-0')\n",
    "    # tf.get_collection() returns a list. In this example we only want the\n",
    "    # first one.\n",
    "    #train_op = tf.get_collection('train_op')[0]\n",
    "    output_op = tf.get_collection('output')[0]\n",
    "    data = tf.get_collection('data')[0]\n",
    "    target = tf.get_collection('target')[0]\n",
    "    initial_state = tf.get_collection('initial_state')[0]\n",
    "    #calc_accuracy(100, output_op, initial_state, data, target)\n",
    "    print(calc_accuracy(100, output_op))\n",
    "    #for step in xrange(1000000):\n",
    "        #sess.run(train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TO DO: \n",
    "- Import the above functionality into the actual code. (add_to_collection and adding saver is the main difference)\n",
    "- do lots of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
